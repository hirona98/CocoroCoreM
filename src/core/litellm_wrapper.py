"""
LiteLLMçµ±åˆç”¨ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹

MemOSã®BaseLLMã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã«æº–æ‹ ã—ã€
LiteLLMã‚’ä½¿ç”¨ã—ã¦ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼LLMå¯¾å¿œã‚’å®Ÿç¾
"""

import logging
import os
from collections.abc import Generator
from typing import Dict, Any, List

logger = logging.getLogger(__name__)


class LiteLLMConfig:
    """LiteLLMè¨­å®šã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model_name: str, api_key: str, 
                 extra_config: Dict[str, Any] = None, **kwargs):
        self.model_name_or_path = model_name
        self.api_key = api_key
        self.max_tokens = kwargs.get('max_tokens', 1024)
        self.remove_think_prefix = kwargs.get('remove_think_prefix', False)
        self.extra_config = extra_config or {}
        
        # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åã‚’è‡ªå‹•æŠ½å‡ºï¼ˆä¾‹: "xai/grok-2-latest" â†’ "xai"ï¼‰
        self.provider = model_name.split('/')[0] if '/' in model_name else 'openai'


class LiteLLMWrapper:
    """
    LiteLLMãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹
    
    MemOSã®BaseLLMã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’å®Ÿè£…ã—ã€
    LiteLLMã‚’ä½¿ç”¨ã—ã¦ãƒãƒ«ãƒãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼LLMæ©Ÿèƒ½ã‚’æä¾›
    """
    
    def __init__(self, config: LiteLLMConfig):
        """
        åˆæœŸåŒ–
        
        Args:
            config: LiteLLMè¨­å®š
        """
        self.config = config
        self.logger = logger
        
        # ç’°å¢ƒå¤‰æ•°è¨­å®šï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ï¼‰
        self._setup_environment_variables()
        
        # LiteLLMã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆé…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰
        try:
            import litellm
            self.litellm = litellm
            # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®šï¼ˆLiteLLMã®è©³ç´°ãƒ­ã‚°ã‚’æŠ‘åˆ¶ï¼‰
            litellm.set_verbose = False
        except ImportError:
            raise RuntimeError("LiteLLMãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install litellm ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
        
        logger.info(f"LiteLLMWrapperåˆæœŸåŒ–å®Œäº†: model={self.config.model_name_or_path}")
    
    def _setup_environment_variables(self):
        """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ç’°å¢ƒå¤‰æ•°è¨­å®š"""
        if self.config.api_key:
            if self.config.provider == "openai":
                os.environ["OPENAI_API_KEY"] = self.config.api_key
            elif self.config.provider == "anthropic":
                os.environ["ANTHROPIC_API_KEY"] = self.config.api_key
            elif self.config.provider == "xai":
                os.environ["XAI_API_KEY"] = self.config.api_key
            elif self.config.provider == "vertex_ai":
                # Vertex AIã®å ´åˆã¯è¿½åŠ è¨­å®šãŒå¿…è¦
                if "project_id" in self.config.extra_config:
                    os.environ["VERTEXAI_PROJECT"] = self.config.extra_config["project_id"]
                if "location" in self.config.extra_config:
                    os.environ["VERTEXAI_LOCATION"] = self.config.extra_config["location"]
            # ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚‚åŒæ§˜ã«è¿½åŠ å¯èƒ½
    
    def generate(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """
        LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¾‹å¤–ã‚’å†ç™ºç”Ÿï¼‰
        
        Args:
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
            **kwargs: è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Returns:
            str: ç”Ÿæˆã•ã‚ŒãŸãƒ¬ã‚¹ãƒãƒ³ã‚¹
        """
        try:
            # LiteLLM completionå‘¼ã³å‡ºã—
            response = self.litellm.completion(
                model=self.config.model_name_or_path,
                messages=messages,
                max_tokens=self.config.max_tokens,
                **self.config.extra_config,  # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰è¨­å®š
                **kwargs  # å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            )
            
            response_content = response.choices[0].message.content
            
            # ãƒ­ã‚°å‡ºåŠ›ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
            logger.debug(f"LiteLLM Response: model={response.model}, usage={response.usage}")
            
            return response_content
                
        except Exception as e:
            # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å‡ºåŠ›
            self._log_detailed_error(e, "generate", messages, kwargs)
            # ã‚¨ãƒ©ãƒ¼ã‚’å†ç™ºç”Ÿï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ãªã„ï¼‰
            raise
    
    def generate_stream(self, messages: List[Dict[str, str]], **kwargs) -> Generator[str, None, None]:
        """
        ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã¯ä¾‹å¤–ã‚’å†ç™ºç”Ÿï¼‰
        
        Args:
            messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
            **kwargs: è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            
        Yields:
            str: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒ³ã‚¯
        """
        try:
            # LiteLLM completionï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰å‘¼ã³å‡ºã—
            response = self.litellm.completion(
                model=self.config.model_name_or_path,
                messages=messages,
                stream=True,  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æœ‰åŠ¹
                max_tokens=self.config.max_tokens,
                **self.config.extra_config,  # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰è¨­å®š
                **kwargs  # å‹•çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            )
            
            reasoning_started = False
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹å‡¦ç†
            for chunk in response:
                if not chunk.choices or not chunk.choices[0].delta:
                    continue
                    
                delta = chunk.choices[0].delta
                
                # reasoning_contentå¯¾å¿œï¼ˆDeepSeekç­‰ï¼‰
                if hasattr(delta, "reasoning_content") and delta.reasoning_content:
                    if not reasoning_started and not self.config.remove_think_prefix:
                        yield "<think>"
                        reasoning_started = True
                    if not self.config.remove_think_prefix:
                        yield delta.reasoning_content
                
                # é€šå¸¸ã®content
                elif hasattr(delta, "content") and delta.content:
                    if reasoning_started and not self.config.remove_think_prefix:
                        yield "</think>"
                        reasoning_started = False
                    yield delta.content
            
            # thinking ã‚¿ã‚°çµ‚äº†å‡¦ç†
            if reasoning_started and not self.config.remove_think_prefix:
                yield "</think>"
                
        except Exception as e:
            # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å‡ºåŠ›
            self._log_detailed_error(e, "generate_stream", messages, kwargs)
            # ã‚¨ãƒ©ãƒ¼ã‚’å†ç™ºç”Ÿï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ãªã„ï¼‰
            raise
    
    def _log_detailed_error(self, error: Exception, operation: str, 
                           messages: List[Dict[str, str]], kwargs: Dict[str, Any]):
        """è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        error_info = {
            "operation": operation,
            "provider": self.config.provider,
            "model": self.config.model_name_or_path,
            "error_type": type(error).__name__,
            "error_message": str(error),
            "message_count": len(messages),
            "kwargs": {k: v for k, v in kwargs.items() if k not in ['api_key']},  # APIã‚­ãƒ¼ã‚’é™¤å¤–
        }
        
        # ã‚¨ãƒ©ãƒ¼åˆ†é¡ã¨è©³ç´°ãƒ­ã‚°
        error_str = str(error)
        if "401" in error_str or "Authentication" in error_str:
            logger.error(f"ğŸ” èªè¨¼ã‚¨ãƒ©ãƒ¼ - APIã‚­ãƒ¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {error_info}")
        elif "429" in error_str or "rate limit" in error_str.lower():
            logger.error(f"â±ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ - ä½¿ç”¨é‡ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {error_info}")
        elif "timeout" in error_str.lower() or "TimeoutError" in error_str:
            logger.error(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ - ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çŠ¶æ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {error_info}")
        elif any(code in error_str for code in ["502", "503", "504"]):
            logger.error(f"ğŸ–¥ï¸ ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼ - ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å´ã®ä¸€æ™‚çš„ãªå•é¡Œã®å¯èƒ½æ€§: {error_info}")
        elif "quota" in error_str.lower() or "limit" in error_str.lower():
            logger.error(f"ğŸ’³ ã‚¯ã‚©ãƒ¼ã‚¿ãƒ»åˆ¶é™ã‚¨ãƒ©ãƒ¼ - ä½¿ç”¨åˆ¶é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„: {error_info}")
        else:
            logger.error(f"â“ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {error_info}", exc_info=True)